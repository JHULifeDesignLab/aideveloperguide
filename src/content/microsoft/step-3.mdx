---
title: "Step 3 · Enterprise AI & Governance"
vendor: "microsoft"
ordinal: 3
slug: "/microsoft/step-3"
lastReviewed: "2025-08-06"
---

# Step 3: Advanced Responsible AI — Model Documentation & Risk Awareness 

## Goal  
Create a concise **responsible AI README** for your project that a recruiter/teammate can skim in 2–3 minutes.
Align it with [Microsoft’s Responsible AI principles](https://www.microsoft.com/en-us/ai/principles-and-approach) and make clear how you mitigate risks (prompt rules, Content Safety thresholds, evaluation checklist, fallback UX, logging policies).

Use the example documentation below to write a README for the project you created in Step 2.

## Example Documentation
**1) System & Models**  
- **Platform:** Azure AI Foundry (project + deployment).  
- **Model(s):** Azure OpenAI chat model for generation; embeddings for retrieval.  
- **RAG:** Azure AI Search index; hybrid retrieval (keyword + vector).  

**2) Intended Use / Users**  
- Designed for *X* scenario; *not* for medical, legal, or safety-critical advice.

**3) Data & Indexing**  
- Source docs (owner, license), chunking approach, update cadence, PII handling.

**4) Safety & Guardrails**  
- **Azure AI Content Safety** thresholds (hate/sexual/violence/self-harm).  
- Prompt rules (refusal patterns, “no training data claims”, escalation).  
- Blocklists/allowlists if applicable.

**5) Limitations / Failure Modes**  
- Hallucination risk when outside indexed scope; latency under load; ambiguity handling.

**6) Evaluation & Quality**  
- Groundedness checks on a seed set; spot-check answer/citation pairs; track regressions.

**7) Operations**  
- Rate limits/quotas; cost guardrails; telemetry (requests, errors, refusals).  
- Privacy note (keys in Key Vault; no logging of sensitive user content by default).

Add this file to the repo and link it from your README.
