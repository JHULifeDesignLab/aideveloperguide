---
title: "Step 3 · Responsible AI & Model Documentation"
vendor: "amazon"
ordinal: 3
slug: "/amazon/step-3"
lastReviewed: "2025-08-06"
---

# Step 3: Responsible AI on AWS · Model Cards & Guardrails

## Goal
Show that you can ship responsibly by documenting limitations and adding safety controls to your app.

### 1) Create a Model Card (SageMaker)
Use **Amazon SageMaker Model Cards** to capture: intended use, data sources, known risks (e.g., bias, hallucinations), metrics, and caveats. You can export to PDF and link it in your repo. Read the [documentation here.](https://docs.aws.amazon.com/sagemaker/latest/dg/model-cards.html)

### 2) Add Guardrails (Bedrock)
Configure **Bedrock Guardrails** to filter harmful content, redact sensitive info, and enforce denied topics. You can attach guardrails to Agents *or* call the `ApplyGuardrail` API independently to pre/post‑filter inputs/outputs.
* [Guardrails guide](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)
* [ApplyGuardrail API](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-independent-api.html)

### 3) Document it in your README
Add a **Responsible AI** section that says:
* What the app does and doesn’t do
* How retrieval and citations work
* Which guardrails are enabled and why
* Known failure modes and how users should report issues

## Example documentation snippets
*Resume bullet:* “Built a campus FAQ chatbot on Amazon Bedrock with Knowledge Bases; implemented Guardrails for topic filtering and PII redaction; published a SageMaker Model Card and README detailing limits and evaluation results.”

*LinkedIn post:* “Shipped a small RAG app on AWS! Used Bedrock to ground answers in campus docs, enabled Guardrails, and wrote a Model Card to be transparent about risks. Feedback welcome.”